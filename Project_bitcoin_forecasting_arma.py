# -*- coding: utf-8 -*-
"""GITHUB_BitCoin_Forecasting_ARMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cJCxfpeirtD2DB0mWTPVyJ7mYPJxrOO

## Dataset

Bitcoin is a digital currency created in January 2009. It follows the ideas set out in a whitepaper by the mysterious and pseudonymous Satoshi Nakamoto. The identity of the person or persons who created the technology is still a mystery. Bitcoin offers the promise of lower transaction fees than traditional online payment mechanisms and, unlike government-issued currencies, it is operated by a decentralized authority.

Data Description
This dataset provides the history of daily prices of Bitcoin. The data starts from 17-Sep-2014 and is updated till 09-July-2021. All the column descriptions are provided below.

* Date: Day/Month/Year
* Open: Price from the first transaction of a trading day
* High: Maximum price in a trading day
* Low: Minimum price in a trading day
* Close: Price from the last transaction of a trading day
* Adj Close: Closing price adjusted to reflect the value after accounting for any corporate actions
* Volume: Number of units traded in a day
"""

#@title Download Dataset
!wget -qq !wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/BTC.csv
print("Dataset downloaded successfully!!")

"""### Import required Packages"""

!pip install -q mplfinance

import warnings
warnings.simplefilter('ignore')
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from math import sqrt
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.graphics.tsaplots import plot_predict
import statsmodels.api as sm
import itertools
import mplfinance as mpf

"""### Load the data and perform EDA

"""

btc_df = pd.read_csv('BTC.csv', index_col='Date', parse_dates=True)
btc_df.head()

btc_df.shape

btc_df.isnull().sum()

btc_df.dropna(inplace=True)
print(btc_df.shape)

btc_df.describe()

# plotting ohlc and volume data using mplfinance
kws=dict(volume=True,tight_layout=True, style='yahoo', type='candle', figsize=(12,8))
mpf.plot(btc_df,**kws)

rs_df = btc_df.resample('w').mean()
rs_df.head()

mpf.plot(rs_df,**kws)

periods = [5,9,12,14,26,200]
plt.subplots(figsize=(15,6))
for i,t in enumerate(periods):
    plt.subplot(len(periods)//2,3,i+1)
    pd.plotting.lag_plot(btc_df['Close'], lag=t)
    plt.title('Lag '+str(periods[i]))
    plt.tight_layout()

plt.show()

"""### Feature engineering
Bollinger bands, MACD and Stachastic momentum are important features for the analysis
"""

# Bollinger-Band
# computes the datapoints for moving average, upper band and the lower band
# source: https://github.com/pratiknabriya/Bollinger-Bands-with-Python/blob/main/Bollinger_Bands.ipynb
def bollinger_band(df, window = 5, num_stdev = 2):
    length = window
    bb = pd.DataFrame()
    prices = df['Close']
    bb['MA'] = prices.rolling(length).mean()
    bb['stdev2'] = prices.rolling(length).std()
    bb['upband'] = bb['MA'] + num_stdev*bb['stdev2']
    bb['dwnband'] = bb['MA'] - num_stdev*bb['stdev2']
    bb['PercentB'] = (df['Close']-bb['dwnband'])/(bb['upband']-bb['dwnband'])
    return bb

bb_df = bollinger_band(btc_df)

bb_df.head(7)

fy_20_21_df = btc_df.loc['2020-04-01':'2021-03-31'].copy()
bb_df = bollinger_band(fy_20_21_df, window=5)  # DataFrame with Moving average columns
apd  = mpf.make_addplot(bb_df['MA'],
                        fill_between=dict(y1=bb_df['dwnband'].values,
                                          y2=bb_df['upband'].values,
                                          color='grey',
                                          alpha=0.3),)
mpf.plot(fy_20_21_df,**kws, addplot=apd)

# Moving Average Convergence Divergence (MACD)
def MACD(df, window_slow, window_fast, window_signal):
    macd = pd.DataFrame()
    macd['ema_slow'] = df['Close'].ewm(span=window_slow).mean()
    macd['ema_fast'] = df['Close'].ewm(span=window_fast).mean()
    macd['macd'] = macd['ema_slow'] - macd['ema_fast']
    macd['signal'] = macd['macd'].ewm(span=window_signal).mean()
    macd['diff'] = macd['macd'] - macd['signal']
    macd['bar_positive'] = macd['diff'].map(lambda x: x if x > 0 else 0)
    macd['bar_negative'] = macd['diff'].map(lambda x: x if x < 0 else 0)
    return macd

macd = MACD(fy_20_21_df, 12, 26, 9)
macd_plot  = [
    mpf.make_addplot((macd['macd']), color='#606060', panel=2, ylabel='MACD', secondary_y=False),
    mpf.make_addplot((macd['signal']), color='#1f77b4', panel=2, secondary_y=False),
    mpf.make_addplot((macd['bar_positive']), type='bar', color='#4dc790', panel=2),
    mpf.make_addplot((macd['bar_negative']), type='bar', color='#fd6b6c', panel=2),
]

mpf.plot(fy_20_21_df, **kws, addplot=macd_plot)

# Stochastic-Oscillator
def Stochastic(df, window, smooth_window):
    stochastic = pd.DataFrame()
    stochastic['%K'] = ((df['Close'] - df['Low'].rolling(window).min()) \
                        / (df['High'].rolling(window).max() - df['Low'].rolling(window).min())) * 100
    stochastic['%D'] = stochastic['%K'].rolling(smooth_window).mean()
    stochastic['%SD'] = stochastic['%D'].rolling(smooth_window).mean()
    stochastic['UL'] = 80
    stochastic['DL'] = 20
    return stochastic

stochastic = Stochastic(fy_20_21_df, 14, 3)
stochastic_plot  = [
    mpf.make_addplot((stochastic[['%K', '%D', '%SD', 'UL', 'DL']]), ylim=[0, 100], panel=2, ylabel='S.O.')
]

mpf.plot(fy_20_21_df, **kws, addplot=stochastic_plot)

# putting all together for entry and exit decisions
apd  = [mpf.make_addplot(bb_df['MA'],
                        fill_between=dict(y1=bb_df['dwnband'].values,
                                          y2=bb_df['upband'].values,
                                          color='grey',
                                          alpha=0.3),),
        mpf.make_addplot((macd['macd']), color='#606060', panel=2, ylabel='MACD', secondary_y=False),
        mpf.make_addplot((macd['signal']), color='#1f77b4', panel=2, secondary_y=False),
        mpf.make_addplot((macd['bar_positive']), type='bar', color='#4dc790', panel=2),
        mpf.make_addplot((macd['bar_negative']), type='bar', color='#fd6b6c', panel=2),
        mpf.make_addplot((stochastic[['%K', '%D', '%SD', 'UL', 'DL']]), ylim=[0, 100], panel=3, ylabel='S.O.'),
        ]
mpf.plot(fy_20_21_df,**kws, addplot=apd)

"""#### Analyze the correlation (heatmap) of all the features"""

g = sns.pairplot(btc_df[btc_df.columns])

btc_df_2 = btc_df.copy()
bb_df_2 = bollinger_band(btc_df_2)
macd_df_2 = MACD(btc_df_2, 12, 26, 9)
so_df_2 = Stochastic(btc_df_2, 14, 3)
btc_df_2 = pd.concat([btc_df_2, bb_df_2, macd_df_2, so_df_2], axis=1)

btc_df_2.info()

btc_df_2 = btc_df_2.drop(columns=['UL','DL'])

btc_pear_corr= btc_df_2[btc_df_2.columns].corr(method='pearson')
btc_pear_corr

plt.figure(figsize=(15,15))
sns.heatmap(btc_pear_corr,annot=True)

"""### Feature Importance
Using RandomForestRegressor for identifying feature importance
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

feat_df = btc_df_2.copy()
feat_df.fillna(0,inplace=True)
# scale the feat_df
scaler = StandardScaler()
feat_df = pd.DataFrame(scaler.fit_transform(feat_df), columns=feat_df.columns)

feat_df.head()

X, y = feat_df.drop(columns=['Close']), feat_df['Close']

clf = RandomForestRegressor(n_estimators=100, random_state=42)
clf.fit(X, y)

importance = clf.feature_importances_

plt.barh(X.columns,importance)
plt.xticks(rotation=90)
plt.title('Feature Importance')
plt.show()

features=['Close','ema_fast']
plt.figure(figsize=(5,3))
sns.heatmap(btc_df_2[features].corr(),annot=True)

"""As there are two important features that could predict the closing price
- Closing Price historical data
- Exponential moving average of the btc dataset.

we could use two separate models to predict the prices and compare it.

### Test the Stationarity using Augmented Dicky Fuller Test

The Augmented Dicky Fuller test is a type of statistical test called a unit root test.
"""

ts1 = btc_df['Close'].copy()
ts1.fillna(0, inplace=True)

#Perform Dickey Fuller test
def test_stationarity(timeseries):
    print('alpha = 0.05')
    print('Results of Dickey Fuller Test:')
    result = adfuller(timeseries)
    print('ADF Stastistic: %f'%result[0])
    print('p-value: %f'%result[1])
    pvalue = result[1]
    for key,value in result[4].items():
        if result[0]>value:
            print("The graph is non stationery")
            break
        else:
            print("The graph is stationary")
            break;
        print('Critical values:')
    for key,value in result[4].items():
            print('\t%s: %.3f ' % (key, value))
    print('-----End of Test-----')

test_stationarity(ts1)

"""### Identify the trends and seasonality from the given time series data

* Apply seasonal decompose and plot the results
* Check the stationarity of data using rolling mean and rolling standard deviation.
* Make the time series data stationary
  * Apply a log transformation to reduce the variance of the series
  * Eliminate the Trend and Seasonality by Differencing

"""

ts1 = btc_df['Close'].copy()

plt.figure(figsize=(5,2))
ts1.plot(kind='hist')
plt.legend(loc='best')
plt.show()

# ETS Decomposition
decompose_monthly = seasonal_decompose(ts1, model='additive', period=30)
# ETS plot
decompose_monthly.plot()
plt.show()

# ETS Decomposition
decompose_yearly = seasonal_decompose(ts1, model='additive', period=365)
# ETS plot
decompose_yearly.plot()
plt.show()

logts1 = ts1.apply(lambda x: np.log(x))
logts1.fillna(0, inplace=True)
logts1.head()

logts1.info()

plt.figure(figsize=(5,2))
logts1.plot(kind='hist')
plt.legend(loc='best')
plt.show()

rolling_mean = logts1.rolling(window=365).mean()
rolling_std = logts1.rolling(window=365).std()

plt.figure(figsize=(15,6))
plt.plot(logts1, label='Log(Close)')
plt.plot(rolling_mean, color='red', label='Rolling Mean')
plt.plot(rolling_std, color='black', label='Rolling Std')
plt.legend(loc='best')
plt.title('log_Close')
plt.show()

"""The plot suggests the `logts1` is non-stationary."""

# Performing ADF test
test_stationarity(logts1)

# performing ACF and PACF
f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15,6))
plot_acf(ts1, lags=250, ax= ax[0])
plot_pacf(ts1, lags=25, ax= ax[1])
plt.show()

"""Observations from ACF and PACF of `ts1`

|graph|comments|
|-----|--------|
|ACF|Lags upto `t+140` is significant in prediction of the price|
|PACF|`t+1` lag has direct significance|
"""

# detrending by differencing
ts1_diff = ts1 - ts1.shift(1)
ts1_diff.fillna(0, inplace=True)
ts1_diff.head()

plt.figure(figsize=(5,2))
ts1_diff.plot(kind='hist')
plt.legend(loc='best')
plt.show()

rolling_mean = ts1_diff.rolling(window=14).mean()
rolling_std = ts1_diff.rolling(window=14).std()

plt.figure(figsize=(15,6))
plt.plot(ts1_diff, label='Diffencing signal')
plt.plot(rolling_mean, color='red', label='Rolling Mean')
plt.plot(rolling_std, color='black', label='Rolling Std')
plt.legend(loc='best')
plt.title('Close_diff_shift(1)')
plt.show()

"""The plot suggests `ts1_diff` is stationary. Although it is not conclusive!

### Test the Stationarity using Augmented Dicky Fuller Test

Verify the stationarity post differencing, using ADF
"""

test_stationarity(ts1_diff)

"""### Auto Correlation Plot

Autocorrelation refers to the degree of correlation between the values of the same variables across different observations in the data.  The concept of autocorrelation is most often discussed in the context of time series data in which observations occur at different points in time.

* Plot the auto correlation function (ACF and PACF)
* Analyse ACF and PACF plots and define AR (p) and MA(q) terms
"""

# ACF and PACF plots

f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15,6))
plot_acf(ts1_diff, lags=250, ax= ax[0])
plot_pacf(ts1_diff, lags=50, ax= ax[1])
plt.show()

"""Based on the above plots for estimation using `['Close']` price:

|model|value|
|-----|-----|
|AR(p)|12|
|MA(q)|1|

### Train the Auto Regressive model
"""

# Train test split
train_data = ts1_diff[:'2019']
test_data = ts1_diff['2020':'2020']
train_start = train_data.index.min()
train_end = train_data.index.max()
test_start = test_data.index.min()
test_end = test_data.index.max()

print(train_start,train_end)
print(test_start,test_end)

train_data.tail()

test_data.head()

ar_model0 = sm.tsa.AutoReg(train_data,lags=0)
ar_model0_fit = ar_model0.fit()
print(ar_model0_fit.summary())
print(ar_model0_fit.params)

plt.figure(figsize=(15,3))
plot_predict(ar_model0_fit,start=test_start,end=test_end)
plt.plot(test_data[test_start:test_end])
plt.show()

fig = plt.figure(figsize=(16,9))
fig = ar_model0_fit.plot_diagnostics(fig=fig, lags=10)

"""### ARMA Model

* Train the ARMA model on timeseries data and print AIC value
"""

arma_model10_1 = ARIMA(train_data,order=(10,1,0))
arma_model10_1_fit = arma_model10_1.fit()
print(arma_model10_1_fit.summary())

fig = plt.figure(figsize=(16,9))
fig = arma_model10_1_fit.plot_diagnostics(fig=fig, lags=20)

plt.figure(figsize=(15,3))
plot_predict(arma_model10_1_fit,start=test_start,end=test_end)
plt.plot(test_data[test_start:test_end])
plt.show()

"""#### Plot the predictions for timeseries data"""

previous_close = ts1.loc['2020':].shift(-1)
previous_close.fillna(0, inplace=True)
print(previous_close)

# predicting using ARMA model
from statsmodels.tsa.base.prediction import PredictionResults

pred = arma_model10_1_fit.get_prediction(start=test_start, end=test_end, dynamic=True)
yhat = pred.predicted_mean
ci = pred.conf_int(alpha=0.05)
print(pred.predicted_mean,pred.conf_int(alpha=0.05))

print(ci['lower Close'])

def transform_timeseries_pred(data,close_price):
    ts = data.copy()
    ts.fillna(0, inplace=True)
    return ts+close_price

a_yhat = transform_timeseries_pred(yhat,previous_close)
a_ci_low = transform_timeseries_pred(ci['lower Close'],previous_close)
a_ci_high = transform_timeseries_pred(ci['upper Close'],previous_close)
print(a_yhat, a_ci_low, a_ci_high)

plt.figure(figsize=(15,3))
plt.plot(a_yhat.index.values, a_yhat, '.r', markersize= 5, label='Forecast mean',
         alpha= 0.3)
plt.fill_between(
    a_ci_low.index.values,
    a_ci_low,
    a_ci_high,
    color="gray",
    alpha = 0.3,
    label = "95% confidence interval",
)
plt.plot(ts1.loc['2020':'2020'], label='Actual')
plt.legend(loc='best')
plt.show()

"""#### Effect of sudden change in timeseries-data-set on ARMA parameters

The dataset from 01 Jan 2021 onwards show a huge change in the closing prices in a short duration. We will study the effect of the coefficients on the ARMA model by training on this dataset.
"""

plt.figure(figsize=(15,3))
plt.plot(ts1['2021':])
plt.show()

# Copying the values of 2021 data
ts2 = ts1['2021':].copy()

# Differencing to obtain stationarity of the data
ts2_diff = ts2 - ts2.shift(1)
ts2_diff.fillna(0, inplace=True)

# test train split and making previous close data
train_set = ts2_diff[:'2021-07-01']
test_set = ts2_diff['2021-07-01':]
previous_close = ts2.shift(-1)
previous_close.fillna(0, inplace=True)

test_set.tail()

# Model training
_arma_model10_1 = ARIMA(train_set,order=(10,1,0))
_arma_model10_1_fit = _arma_model10_1.fit()
print(_arma_model10_1_fit.summary())

print(arma_model10_1_fit.summary())

# predicting using ARMA model
pred = arma_model10_1_fit.get_prediction(start='2021-07-01', end='2021-08-10', dynamic=True)
yhat = pred.predicted_mean
ci = pred.conf_int(alpha=0.05)

# Transforming the timeseries from predictions
a_yhat = transform_timeseries_pred(yhat,previous_close)
a_ci_low = transform_timeseries_pred(ci['lower Close'],previous_close)
a_ci_high = transform_timeseries_pred(ci['upper Close'],previous_close)

plt.figure(figsize=(15,3))
plt.plot(a_yhat.index.values, a_yhat, '.r', markersize= 5, label='Forecast mean',
         alpha= 0.3)
plt.fill_between(
    a_ci_low.index.values,
    a_ci_low,
    a_ci_high,
    color="gray",
    alpha = 0.3,
    label = "95% confidence interval",
)
plt.plot(ts2, label='Actual')
plt.legend(loc='best')
plt.show()

"""### Report Analysis

* Discuss how sudden effects of bitcoin price affects the model parameters
* State your observations about the trend and seasonality of the timeseries data
* Discuss how you selected ARMA (p and q) terms.
* Interpret the AIC value obtained for the ARMA model

### 1. Effects of Bitcoin Price Changes on Model Parameters
Looking at the two ARIMA models trained on different periods:
- First model (2014-2019): Trained on relatively stable price movements
- Second model (2021): Trained during volatile price period with dramatic increases

### 2. Key observations:
- The coefficients in the 2021 model show much larger absolute values, indicating higher sensitivity to price changes
- The standard errors of the coefficients are significantly larger in the 2021 model, showing increased uncertainty
- The model trained on volatile 2021 data has poorer predictive power, visible in the wider confidence intervals in the plots

### 3. Trend and Seasonality Analysis
- From the seasonal decomposition analysis shown in the code:

Trend:
- Strong upward trend over the entire period
- Non-linear growth pattern
- Particularly steep increase in 2021

Seasonality:

- Two decompositions were performed:
 - Monthly (period=30)
 - Yearly (period=365)

The seasonal patterns are relatively weak compared to the trend
No clear consistent seasonal pattern emerged, suggesting Bitcoin prices are more driven by market events than seasonal factors

### 3. Selection of ARMA Terms (p,q)

The selection process used ACF and PACF plots:
ACF analysis showed:

- Significant correlations up to lag 140
- Gradual decay pattern

PACF analysis revealed:

- Strong significance at lag 1
- Several significant spikes up to lag 12

Based on these, the model parameters were chosen as:

- AR(p) = 12 (from PACF significant lags)
- MA(q) = 1 (from ACF pattern)

### 4. Interpretation of AIC Value

The ARMA model shows an AIC value of approximately 89149.5. This value tells us:
- The model has moderate complexity given the data
- Lower than simpler models, indicating better fit

However, the relatively high AIC suggests:
- High volatility in the data
- Potential need for additional features or different model structures
- Room for improvement in model specification

Additional information:

The code uses differencing to achieve stationarity, which was confirmed by the Augmented Dickey-Fuller test. This transformation was necessary for the ARMA modeling but also indicates the strong trend component in the original data.

The model's predictive power is notably better in periods of stable price movement compared to volatile periods, suggesting that traditional time series models might need to be supplemented with other approaches for highly volatile periods in cryptocurrency markets.
"""